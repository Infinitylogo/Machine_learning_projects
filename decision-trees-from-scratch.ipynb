{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# ID3 Algorithm"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"0x-3lut9koj8"}},{"cell_type":"markdown","source":["In the ID3 algorithm, decision trees are calculated using the concept of entropy and information gain."],"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"jJ_eXfltkokF"}},{"cell_type":"markdown","source":["#### Entropy can be defined as:\n","\\begin{align}\n","H(S)=-\\sum_{i=1}^{N}p_{i}log_{2}p_{i}\n","\\end{align}"],"metadata":{"_uuid":"a7f59709ffbc212e48c71187546e4fc371d1edd5","id":"LgNERGt_kokG"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","# eps for making value a bit greater than 0 later on\n","eps = np.finfo(float).eps\n","\n","from numpy import log2 as log"],"metadata":{"_uuid":"cb71d83192eecd0990a26e07e192e2b2643dfc8e","trusted":true,"id":"AshVOJ11kokH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Creating a dataset,"],"metadata":{"_uuid":"546c43c0ab26716b6a90f1b9cf804ec517200353","id":"0s2RZzlekokJ"}},{"cell_type":"code","source":["dataset = {'Taste':['Salty','Spicy','Spicy','Spicy','Spicy','Sweet','Salty','Sweet','Spicy','Salty'],\n","       'Temperature':['Hot','Hot','Hot','Cold','Hot','Cold','Cold','Hot','Cold','Hot'],\n","       'Texture':['Soft','Soft','Hard','Hard','Hard','Soft','Soft','Soft','Soft','Hard'],\n","       'Eat':['No','No','Yes','No','Yes','Yes','No','Yes','Yes','Yes']}"],"metadata":{"_uuid":"3e50238a87947434739a0f8fe0ce3f8a4b50cecb","trusted":true,"id":"qCKNqog3kokK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.DataFrame(dataset,columns=['Taste','Temperature','Texture','Eat'])\n","df"],"metadata":{"scrolled":true,"_uuid":"8a26723cf7199c9ffa865339e8ce3c876dbda7e6","trusted":true,"id":"QaM5AfQjkokL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def find_entropy(df):\n","    '''\n","    Function to calculate the entropy of the label i.e. Eat.\n","    '''\n","    Class = df.keys()[-1] \n","    entropy = 0\n","    values = df[Class].unique()\n","    for value in values:\n","        fraction = df[Class].value_counts()[value]/len(df[Class])\n","        entropy += -fraction*np.log2(fraction)\n","    return entropy"],"metadata":{"_uuid":"1f7080be0c81b8a949b3ef178b071f16d4af63e6","trusted":true,"id":"qiWXo-aWkokM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def find_entropy_attribute(df,attribute):\n","    '''\n","    Function to calculate the entropy of all features.\n","    '''\n","    Class = df.keys()[-1]   \n","    target_variables = df[Class].unique()  \n","    variables = df[attribute].unique()\n","    entropy2 = 0\n","    for variable in variables:\n","        entropy = 0\n","        for target_variable in target_variables:\n","                num = len(df[attribute][df[attribute]==variable][df[Class] ==target_variable])\n","                den = len(df[attribute][df[attribute]==variable])\n","                fraction = num/(den+eps)\n","                entropy += -fraction*log(fraction+eps)\n","        fraction2 = den/len(df)\n","        entropy2 += -fraction2*entropy\n","    return abs(entropy2)"],"metadata":{"_uuid":"9a2a08bbcafd6bddd14a06359be73c674d3de0f9","trusted":true,"id":"Hob5LfSEkokN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def find_winner(df):\n","    '''\n","    Function to find the feature with the highest information gain.\n","    '''\n","    Entropy_att = []\n","    IG = []\n","    for key in df.keys()[:-1]:\n","#         Entropy_att.append(find_entropy_attribute(df,key))\n","        IG.append(find_entropy(df)-find_entropy_attribute(df,key))\n","    return df.keys()[:-1][np.argmax(IG)]"],"metadata":{"_uuid":"1d1b10c48bfaa1b90f765f82b20c0b7cbc942700","trusted":true,"id":"XlSQRm-qkokQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_subtable(df, node, value):\n","    '''\n","    Function to get a subtable of met conditions.\n","    \n","    node: Column name\n","    value: Unique value of the column\n","    '''\n","    return df[df[node] == value].reset_index(drop=True)"],"metadata":{"_uuid":"b1a2bd5287447d6c9e2b05ccaa6b974d9ffaf492","trusted":true,"id":"_hpjkGLskokR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def buildTree(df,tree=None): \n","    '''\n","    Function to build the ID3 Decision Tree.\n","    '''\n","    Class = df.keys()[-1]  \n","    #Here we build our decision tree\n","\n","    #Get attribute with maximum information gain\n","    node = find_winner(df)\n","    \n","    #Get distinct value of that attribute e.g Salary is node and Low,Med and High are values\n","    attValue = np.unique(df[node])\n","    \n","    #Create an empty dictionary to create tree    \n","    if tree is None:                    \n","        tree={}\n","        tree[node] = {}\n","    \n","   #We make loop to construct a tree by calling this function recursively. \n","    #In this we check if the subset is pure and stops if it is pure. \n","\n","    for value in attValue:\n","        \n","        subtable = get_subtable(df,node,value)\n","        clValue,counts = np.unique(subtable['Eat'],return_counts=True)                        \n","        \n","        if len(counts)==1:#Checking purity of subset\n","            tree[node][value] = clValue[0]                                                    \n","        else:        \n","            tree[node][value] = buildTree(subtable) #Calling the function recursively \n","                   \n","    return tree"],"metadata":{"_uuid":"0b82b1433ebc70e7938f398fb7d5f69b8e60d052","trusted":true,"id":"0zd03iGZkokT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tree = buildTree(df)"],"metadata":{"_uuid":"80acb543ad87fe8c6a6e4c19ba861e88e8608553","trusted":true,"id":"NJoPxPW4kokU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The tree splits are as follows,"],"metadata":{"_uuid":"6175d9bfa7bd4283ac33b7d1c23d2257c79d23e4","id":"P9_SjyHpkokU"}},{"cell_type":"code","source":["import pprint\n","pprint.pprint(tree)"],"metadata":{"_uuid":"73d0519d38c49c1393099ef6ddc766e026dd5abb","trusted":true,"id":"SpJnmpkqkokV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, for prediction we go through each node of the tree to find the output."],"metadata":{"_uuid":"a3418eda1b64b9185e92f5c3a6416cc76951c91a","id":"j8ZYnTCMkokW"}},{"cell_type":"code","source":["def predict(inst,tree):\n","    '''\n","    Function to predict for any input variable.\n","    '''\n","    #Recursively we go through the tree that we built earlier\n","\n","    for nodes in tree.keys():        \n","        \n","        value = inst[nodes]\n","        tree = tree[nodes][value]\n","        prediction = 0\n","            \n","        if type(tree) is dict:\n","            prediction = predict(inst, tree)\n","        else:\n","            prediction = tree\n","            break;                            \n","        \n","    return prediction"],"metadata":{"_uuid":"226b1bf352aa42d08bf826a67730117964882a5a","trusted":true,"id":"-B5xKYxlkokW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = {'Taste':'Salty','Temperature':'Cold','Texture':'Hard'}"],"metadata":{"_uuid":"cf39cfa5bdd14a3bd6bcd742de479deebe51885e","trusted":true,"id":"R8qt_9ZJkokX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inst = pd.Series(data)"],"metadata":{"_uuid":"d27da57542e4e9936a026be31aa36c2e03a7fd31","trusted":true,"id":"GWroyEMRkokX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction = predict(inst,tree)\n","prediction"],"metadata":{"_uuid":"9887838ddd1180f31448d4603a1c0edb7a79a729","trusted":true,"id":"_hpyAlNmkokY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# C4.5"],"metadata":{"_uuid":"b63e85f940771d9a37df56a22edca4545b765838","trusted":true,"id":"dXyHjGnBkokY"}},{"cell_type":"markdown","source":["http://gabrielelanaro.github.io/blog/2016/03/03/decision-trees.html"],"metadata":{"_uuid":"fdf7b31077283d79fec86f582da1774fd71024ed","id":"mlk_MBGikokZ"}},{"cell_type":"code","source":["# The input values\n","x1 = [0, 1, 1, 2, 2, 2,3,2,1]\n","x2 = [0, 0, 1, 1, 1, 0,2,2,1]\n","# The class\n","y = np.array([0, 0, 0, 1, 1, 0,1,0,1])"],"metadata":{"_uuid":"194ee98a9178e824609de09a377643e01bd9d97e","trusted":true,"id":"Ao3K83jjkokZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def partition(a):\n","    return {c: (a==c).nonzero()[0] for c in np.unique(a)}"],"metadata":{"_uuid":"ceba9ecdc951cd8bc7cb66348d5a548f5d488a31","trusted":true,"id":"zYfnNANSkoka"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def entropy(s):\n","    res = 0\n","    val, counts = np.unique(s, return_counts=True)\n","    freqs = counts.astype('float')/len(s)\n","    for p in freqs:\n","        if p != 0.0:\n","            res -= p * np.log2(p)\n","    return res"],"metadata":{"_uuid":"f389ee8dd20204ec5fc9d9f7cc2fb15a1fe93862","trusted":true,"id":"41s3_Qdykoka"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def mutual_information(y, x):\n","\n","    res = entropy(y)\n","\n","    # We partition x, according to attribute values x_i\n","    val, counts = np.unique(x, return_counts=True)\n","    freqs = counts.astype('float')/len(x)\n","\n","    # We calculate a weighted average of the entropy\n","    for p, v in zip(freqs, val):\n","        res -= p * entropy(y[x == v])\n","\n","    return res"],"metadata":{"_uuid":"e4a8ec778c16a0af7c1a9ecf2ec16e78f7280d47","trusted":true,"id":"hztDNa8xkoka"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def mutual_information(y, x):\n","\n","    res = entropy(y)\n","\n","    # We partition x, according to attribute values x_i\n","    val, counts = np.unique(x, return_counts=True)\n","    freqs = counts.astype('float')/len(x)\n","\n","    # We calculate a weighted average of the entropy\n","    for p, v in zip(freqs, val):\n","        res -= p * entropy(y[x == v])\n","\n","    return res"],"metadata":{"_uuid":"90ec92b27fa788c8ec30ffc0d75ef87f369df4fc","trusted":true,"id":"xnSoYXWMkokb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pprint import pprint\n","\n","def is_pure(s):\n","    return len(set(s)) == 1\n","\n","def recursive_split(x, y):\n","    # If there could be no split, just return the original set\n","    if is_pure(y) or len(y) == 0:\n","        return y\n","\n","    # We get attribute that gives the highest mutual information\n","    gain = np.array([mutual_information(y, x_attr) for x_attr in x.T])\n","    selected_attr = np.argmax(gain)\n","\n","    # If there's no gain at all, nothing has to be done, just return the original set\n","    if np.all(gain < 1e-6):\n","        return y\n","\n","\n","    # We split using the selected attribute\n","    sets = partition(x[:, selected_attr])\n","\n","    res = {}\n","    for k, v in sets.items():\n","        y_subset = y.take(v, axis=0)\n","        x_subset = x.take(v, axis=0)\n","\n","        res[\"x_%d = %d\" % (selected_attr, k)] = recursive_split(x_subset, y_subset)\n","\n","    return res\n","\n","X = np.array([x1, x2]).T"],"metadata":{"_uuid":"4ea45d6f1e398a0a4f8ceac8f0a35075a44416f7","trusted":true,"id":"C-ikKOq1kokb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["c4_5 = recursive_split(X, y)"],"metadata":{"_uuid":"5e7a389222484d2129d858759b6fa4ddeeaff9db","trusted":true,"id":"a2qUIW6Wkokc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pretty(d, indent=0):\n","    for key, value in d.items():\n","        print('\\t' * indent + str(key))\n","\n","        if isinstance(value, dict):\n","            pretty(value, indent+1)\n","        else:\n","            print('\\t' * (indent+1) + str(value))"],"metadata":{"_uuid":"dbc926740abd5f5c5dfce3236dde25afddbf3de1","trusted":true,"id":"DwglqFB_kokc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pretty(c4_5)"],"metadata":{"_uuid":"33d0e833c10ca1a289708e0394480a0b24d81dfb","trusted":true,"id":"2evqI66Tkokc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hellinger Distance"],"metadata":{"_uuid":"09c00fb1fd4834364728b2db3594ff49345077bd","id":"6nn_IcnCkokd"}},{"cell_type":"markdown","source":["http://www.pythonexample.com/code/hellinger-distance-decision-tree/\n","\n","https://gist.github.com/larsmans/3116927\n","\n","http://videolectures.net/ecmlpkdd08_cieslak_ldtf/?q=hellinger%20distance"],"metadata":{"_uuid":"2a613b37a4a585d632efb85ac2b8cd4b8f17a1a6","id":"HnFmgfMFkokf"}},{"cell_type":"markdown","source":["Three ways of computing the Hellinger distance between two discrete\n","probability distributions using NumPy and SciPy."],"metadata":{"_uuid":"615551416fe243fa1677cff558724b82862ab62b","id":"0JRaGZlIkokg"}},{"cell_type":"code","source":["import time"],"metadata":{"_uuid":"ae5d37eaf79f2057afe782e006638773781b0d73","trusted":true,"id":"XPbLsiKHkokg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from scipy.linalg import norm\n","from scipy.spatial.distance import euclidean\n"," \n","_SQRT2 = np.sqrt(2)     # sqrt(2) with default precision np.float64\n","\n","def hellinger_dist(p, q):\n","    return np.sqrt(np.sum((np.sqrt(p) - np.sqrt(q)) ** 2)) / _SQRT2"],"metadata":{"_uuid":"aeabdbd59186857cd72ddc05b57b93a773a18700","trusted":true,"id":"M-HOVtLokokg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def hellinger_dist(p, q):\n","    return np.sqrt(np.sum((np.sqrt(p) - np.sqrt(q)) ** 2)) / _SQRT2"],"metadata":{"_uuid":"e32cfb283acdd664025caf3064c24b0d5b8aebd9","trusted":true,"id":"5aE5wSw7kokh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The input values\n","x1 = [0, 1, 1, 2, 2, 2, 1]\n","x2 = [0, 0, 1, 1, 1, 0, 1]\n","\n","# The class\n","y = np.array([0, 0, 0, 1, 1, 0, 0])"],"metadata":{"_uuid":"b6b97aaed02c00b60907f20e870a2edbbe59b60a","trusted":true,"id":"8xhlZ9nKkokh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# repeat = 1\n","\n","# p = np.array([.05, .05, .1, .1, .2, .2, .3] * repeat)\n","# q = np.array([  0,   0,  0, .1, .3, .3 ,.3] * repeat)\n","\n","# p /= p.sum()\n","# q /= q.sum()\n","\n","# hellinger_dist(p=p, q=q)"],"metadata":{"_uuid":"4f2ccf4f26b8e2cb318baf28f49675e579565abb","trusted":true,"id":"MT2WR94Skoki"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The input values\n","x1 = [0, 1, 1, 2, 2, 2, 1]\n","x2 = [0, 0, 1, 1, 1, 0, 1]\n","\n","# The class\n","y = np.array([0, 0, 0, 1, 1, 0, 0])"],"metadata":{"_uuid":"90a54cb39de4f84f8048ce0f2670c39084830ecc","trusted":true,"id":"FhPZG6npkoki"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def partition(a):\n","    return {c: (a==c).nonzero()[0] for c in np.unique(a)}"],"metadata":{"_uuid":"70c04eed1cc5430b1496af265a6e8bae33867b8d","trusted":true,"id":"VsHiYQW8koki"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pprint import pprint\n","\n","def is_pure(s):\n","    return len(set(s)) == 1\n","\n","def recursive_split(x, y):\n","    \n","    # If there could be no split, just return the original set\n","    if is_pure(y) or len(y) == 0:\n","        return y\n","\n","    # We get attribute that gives the smallest distance\n","    distance = np.array([hellinger_dist(y/y.sum(), x_attr/x_attr.sum()) for x_attr in x.T])\n","    selected_attr = np.argmin(distance)\n","\n","    # If the distance is very less, nothing has to be done, just return the original set\n","    if np.all(distance < 1e-6):\n","        return y\n","\n","    # We split using the selected attribute\n","    sets = partition(x[:, selected_attr])\n","\n","    res = {}\n","    for k, v in sets.items():\n","        y_subset = y.take(v, axis=0)\n","        x_subset = x.take(v, axis=0)\n","\n","        res[\"x_%d = %d\" % (selected_attr, k)] = recursive_split(x_subset, y_subset)\n","\n","    return res\n","\n","X = np.array([x1, x2]).T\n","hellinger = recursive_split(X, y)"],"metadata":{"_uuid":"2adaf5d8dff0f1d2b4f8ad5d006f9e97c33284de","trusted":true,"id":"KPyARIuWkoki"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pretty_print(d, indent=0):\n","    for key, value in d.items():\n","        print('\\t' * indent + str(key))\n","        if isinstance(value, dict):\n","            pretty(value, indent+1)\n","        else:\n","            print('\\t' * (indent+1) + str(value))"],"metadata":{"_uuid":"e35213daba4b7cac52118812f5762b2c4927b241","trusted":true,"id":"sdiDQv8Lkokj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pretty_print(hellinger)"],"metadata":{"_uuid":"334e7e44bebf3c7c9cab85cde5f78a553143796c","trusted":true,"id":"12UZChTzkokj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"_uuid":"7c0789ec60f0df6c377b6f8cf5e97a99049f50ed","trusted":true,"id":"nx6mZemekokj"},"execution_count":null,"outputs":[]}]}